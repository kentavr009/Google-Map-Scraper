# -*- coding: utf-8 -*-
from __future__ import annotations

import argparse
import csv
import json
import os
import random
import signal
import sys
import threading
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Optional, Tuple

try:
    from .model import Place
    from .scrape_reviews import (
        scrape_place_reviews,
        MAX_RETRIES_PER_PLACE,
        REVIEW_LANGUAGE,
        DEBUG_SELECTORS,
        CODE_VERSION,
    )
except ImportError:
    from placeid.reviews.model import Place  # type: ignore
    from placeid.reviews.scrape_reviews import (  # type: ignore
        scrape_place_reviews,
        MAX_RETRIES_PER_PLACE,
        REVIEW_LANGUAGE,
        DEBUG_SELECTORS,
        CODE_VERSION,
    )

OUT_HEADER = [
    "Beach ID","Place","Category","Categories",
    "Place (UI)","Place URL","Input URL","Lat","Lng",
    "Review ID","Review URL","Rating","Date","Author","Author URL","Author Photo",
    "Is Local Guide","Text","Photo URLs (list)","RawReview"
]

# ---- –ø—Ä–æ—Å—Ç–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–æ–∫—Å–∏ —á–µ—Ä–µ–∑ requests (–±—ã—Å—Ç—Ä–µ–µ, —á–µ–º –ø–æ–¥–Ω–∏–º–∞—Ç—å –±—Ä–∞—É–∑–µ—Ä) ----
def _probe_proxy(proxy_url: str, timeout: float = 8.0) -> bool:
    try:
        import requests
        proxies = {"http": proxy_url, "https": proxy_url}
        r = requests.get("https://www.google.com/generate_204", proxies=proxies, timeout=timeout)
        return r.status_code in (204, 200)
    except Exception:
        return False

def load_places(csv_path: str) -> List[Place]:
    places: List[Place] = []

    def _parse_categories(val: Optional[str]):
        if not val:
            return None
        s = (val or "").strip()
        try:
            arr = json.loads(s)
            if isinstance(arr, list):
                cleaned = [str(x).strip() for x in arr if str(x).strip()]
                return tuple(cleaned) if cleaned else None
        except Exception:
            pass
        s2 = s.strip().strip("[]")
        if not s2:
            return None
        parts = [p.strip().strip('"').strip("'") for p in s2.split(",")]
        parts = [p for p in parts if p]
        return tuple(parts) if parts else None

    with open(csv_path, encoding="utf-8") as f:
        rdr = csv.DictReader(f)
        for r in rdr:
            beach_id = (r.get("Beach ID") or r.get("beach_id") or "").strip() or None
            place_id = (r.get("place_id") or r.get("Place ID") or "").strip()
            name = (r.get("name") or r.get("Place") or "").strip()
            polygon_name = (r.get("polygon_name") or r.get("Polygon") or "").strip() or None
            place_url = (r.get("place_url") or r.get("Place URL") or "").strip() or None

            category = (r.get("category") or r.get("Category") or "").strip() or None
            categories_raw = (r.get("categories") or r.get("Categories") or "").strip() or None
            categories = _parse_categories(categories_raw)

            if place_id and name:
                places.append(Place(
                    place_id=place_id,
                    name=name,
                    place_url=place_url,
                    polygon_name=polygon_name,
                    category=category,
                    categories=categories,
                    beach_id=beach_id,
                ))
            else:
                if DEBUG_SELECTORS:
                    print(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—Å–∫ —Å—Ç—Ä–æ–∫–∏ –±–µ–∑ place_id/name: {r}")

    return places

def load_proxies(file_path: str) -> List[str]:
    if not file_path:
        return []
    if not os.path.exists(file_path):
        print(f"‚ÑπÔ∏è –§–∞–π–ª –ø—Ä–æ–∫—Å–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path} ‚Äî —Ä–∞–±–æ—Ç–∞–µ–º –±–µ–∑ –ø—Ä–æ–∫—Å–∏.")
        return []
    proxies: List[str] = []
    with open(file_path, encoding="utf-8") as f:
        for line in f:
            s = (line or "").strip()
            if not s or s.startswith("#"):
                continue
            proxies.append(s)
    return proxies

def process_one(idx: int,
                place: Place,
                proxy_url: Optional[str],
                lock: threading.Lock,
                writer: csv.DictWriter) -> None:
    acc = []
    last_err: Optional[Exception] = None

    for attempt in range(1, MAX_RETRIES_PER_PLACE + 1):
        try:
            rows = scrape_place_reviews(place, proxy_url=proxy_url, debug=True)
            acc = rows
            break
        except Exception as e:
            last_err = e
            print(f"[{idx}] ERROR {place.name} (try {attempt}/{MAX_RETRIES_PER_PLACE}): {e}")
            time.sleep(min(2.5, 0.7 * attempt))

    if not acc:
        print(f"[{idx}] FAIL {place.name}: –ø—É—Å—Ç–æ –ø–æ—Å–ª–µ {MAX_RETRIES_PER_PLACE} –ø–æ–ø—ã—Ç–æ–∫.")
        return

    with lock:
        for r in acc:
            r2 = dict(r)
            r2.setdefault("Beach ID", getattr(place, "beach_id", None))
            r2.setdefault("Place", getattr(place, "name", None))
            r2.setdefault("Category", getattr(place, "category", None))

            cats = getattr(place, "categories", None)
            if isinstance(cats, tuple):
                r2.setdefault("Categories", json.dumps(list(cats), ensure_ascii=False))
            elif isinstance(cats, list):
                r2.setdefault("Categories", json.dumps(cats, ensure_ascii=False))
            else:
                r2.setdefault("Categories", None)

            if isinstance(r2.get("Photo URLs (list)"), list):
                r2["Photo URLs (list)"] = json.dumps(r2["Photo URLs (list)"], ensure_ascii=False)
            if r2.get("RawReview") is not None:
                try:
                    r2["RawReview"] = json.dumps(r2["RawReview"], ensure_ascii=False)
                except Exception:
                    r2["RawReview"] = None

            writer.writerow(r2)

        try:
            sys.stdout.flush()
        except Exception:
            pass

    print(f"[{idx}] {place.name}: +{len(acc)} –æ—Ç–∑—ã–≤(–æ–≤) (UI lang={REVIEW_LANGUAGE})")

def run_worker(worker_id: int,
               jobs: List[Tuple[int, Place]],
               proxy_url: Optional[str],
               lock: threading.Lock,
               writer: csv.DictWriter,
               allow_no_proxy_fallback: bool = False) -> None:
    if proxy_url:
        ok = _probe_proxy(proxy_url)
        if not ok:
            print(f"üë§ Worker #{worker_id}: –ø—Ä–æ–∫—Å–∏ –ë–ò–¢–´–ô (–ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–µ –ø—Ä–æ—à–ª–∞) ‚Üí "
                  f"{'–ø–µ—Ä–µ—Ö–æ–∂—É –±–µ–∑ –ø—Ä–æ–∫—Å–∏' if allow_no_proxy_fallback else '–ø—Ä–æ–ø—É—â—É –∑–∞–¥–∞–Ω–∏—è'}.")
            if not allow_no_proxy_fallback:
                return
            proxy_url = None

    # –ù–µ–±–æ–ª—å—à–æ–π –¥–∂–∏—Ç—Ç–µ—Ä —Å—Ç–∞—Ä—Ç–∞, —á—Ç–æ–±—ã –Ω–µ –ª—É–ø–∏—Ç—å –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞ –ª–∞–≤–∏–Ω–æ–π —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π
    time.sleep(random.uniform(0.05, 0.6))

    print(f"üë§ Worker #{worker_id}: {'–ø—Ä–æ–∫—Å–∏ = ' + proxy_url if proxy_url else '–±–µ–∑ –ø—Ä–æ–∫—Å–∏'}")
    for idx, place in jobs:
        process_one(idx, place, proxy_url, lock, writer)
        # –º–∏–∫—Ä–æ-–ø–∞—É–∑–∞, —á—Ç–æ–±—ã —Å–Ω–∏–∑–∏—Ç—å —à–∞–Ω—Å ¬´ERR_TUNNEL¬ª –æ—Ç –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞
        time.sleep(random.uniform(0.15, 0.35))

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--in", dest="inp", required=True, help="CSV —Å–æ —Å–ø–∏—Å–∫–æ–º –º–µ—Å—Ç (Beach ID,Place ID,Place,...)")
    parser.add_argument("--out", dest="out", required=True, help="CSV –¥–ª—è –∑–∞–ø–∏—Å–∏ –æ—Ç–∑—ã–≤–æ–≤")
    parser.add_argument("--threads", type=int, default=1, help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Ç–æ–∫–æ–≤")
    parser.add_argument("--proxies", default="proxies.txt", help="–§–∞–π–ª —Å –ø—Ä–æ–∫—Å–∏ (http(s)/socks5(h)://user:pass@host:port)")
    parser.add_argument("--fallback-no-proxy", action="store_true", help="–ï—Å–ª–∏ –ø—Ä–æ–∫—Å–∏ —É–º–µ—Ä, –≤–æ—Ä–∫–µ—Ä –ø—Ä–æ–¥–æ–ª–∂–∏—Ç –±–µ–∑ –ø—Ä–æ–∫—Å–∏")
    args = parser.parse_args()

    places = load_places(args.inp)
    if not places:
        print("–ù–µ—Ç –≤—Ö–æ–¥–Ω—ã—Ö –º–µ—Å—Ç. –ü—Ä–æ–≤–µ—Ä—å CSV.")
        return

    proxies = load_proxies(args.proxies)
    print(f"Scraper version: {CODE_VERSION}")
    print(f"Places: {len(places)}; workers: {args.threads}; output file: {args.out}")

    lock = threading.Lock()
    out_dir = os.path.dirname(os.path.abspath(args.out)) or "."
    os.makedirs(out_dir, exist_ok=True)

    out_f = open(args.out, "a", encoding="utf-8", newline="")
    need_header = (out_f.tell() == 0)
    writer = csv.DictWriter(out_f, fieldnames=OUT_HEADER)
    if need_header:
        writer.writeheader()
        out_f.flush()

    def _sigint_handler(signum, frame):
        print("\n‚õî SIGINT, –∑–∞–∫—Ä—ã–≤–∞—é —Ñ–∞–π–ª‚Ä¶")
        try:
            out_f.flush()
            out_f.close()
        finally:
            os._exit(1)
    try:
        signal.signal(signal.SIGINT, _sigint_handler)
    except Exception:
        pass

    tasks: List[Tuple[int, Place]] = list(enumerate(places, start=1))

    wanted = max(1, int(args.threads))
    if proxies:
        workers = min(wanted, len(proxies))
        if wanted > len(proxies):
            print(f"‚ö†Ô∏è –ó–∞–ø—Ä–æ—à–µ–Ω–æ –ø–æ—Ç–æ–∫–æ–≤: {wanted}, –Ω–æ –ø—Ä–æ–∫—Å–∏: {len(proxies)}. –£—Ä–µ–∂—É –¥–æ {workers}.")
    else:
        workers = wanted

    # –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–π —Å–æ–≤–µ—Ç: –¥–ª—è —ç—Ç–æ–≥–æ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞ –Ω–∞—á–Ω–∏—Ç–µ —Å 3‚Äì4 –≤–æ—Ä–∫–µ—Ä–æ–≤.
    if workers > 4:
        print("‚ÑπÔ∏è –°–æ–≤–µ—Ç: —É–º–µ–Ω—å—à–∏—Ç–µ --threads –¥–æ 3‚Äì4 ‚Äî –º–Ω–æ–≥–∏–µ –ø—Ä–æ–∫—Å–∏-–ø—Ä–æ–≤–∞–π–¥–µ—Ä—ã —Ä–µ–∂—É—Ç –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ—Å—Ç—å.")

    chunks: List[List[Tuple[int, Place]]] = [tasks[i::workers] for i in range(workers)]
    print(f"‚Üí Start {workers} workers{' with proxy' if proxies else ' –±–µ–∑ –ø—Ä–æ–∫—Å–∏'}.")

    try:
        with ThreadPoolExecutor(max_workers=workers) as ex:
            futs = []
            for w_id in range(workers):
                proxy_for_worker = proxies[w_id] if proxies else None
                futs.append(ex.submit(run_worker, w_id, chunks[w_id], proxy_for_worker, lock, writer, args.fallback_no_proxy))
            for _ in as_completed(futs):
                pass
    finally:
        try:
            out_f.flush()
            out_f.close()
        except Exception:
            pass

    print("‚úÖ –ì–æ—Ç–æ–≤–æ.")

if __name__ == "__main__":
    main()
