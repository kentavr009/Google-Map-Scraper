# reviews/main_reviews.py
# -*- coding: utf-8 -*-
from __future__ import annotations

"""
main_reviews.py ‚Äî –º–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω—ã–π –∑–∞–ø—É—Å–∫ –ø–∞—Ä—Å–∏–Ω–≥–∞ –æ—Ç–∑—ã–≤–æ–≤ Google Maps.
–ú–æ–¥–µ–ª—å —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è: worker-per-proxy.
- –ö–∞–∂–¥—ã–π –≤–æ—Ä–∫–µ—Ä (–ø–æ—Ç–æ–∫) –ø–æ–ª—É—á–∞–µ—Ç –û–î–ò–ù —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –ø—Ä–æ–∫—Å–∏ (–µ—Å–ª–∏ –ø—Ä–æ–∫—Å–∏ –∑–∞–¥–∞–Ω—ã)
  –∏ —Å–≤–æ—é –ø–æ—Ä—Ü–∏—é –º–µ—Å—Ç –¥–ª—è –æ–±—Ö–æ–¥–∞.
- –ó–∞–ø–∏—Å—å –≤ CSV ‚Äî –ø–æ–¥ –æ–±—â–∏–º lock.
–°–æ–≤–º–µ—Å—Ç–∏–º–æ —Å –∑–∞–ø—É—Å–∫–æ–º –∫–∞–∫ –ø–∞–∫–µ—Ç–∞: `python -m reviews.main_reviews` –∏ –∫–∞–∫ —Å–∫—Ä–∏–ø—Ç–∞.
"""

import argparse
import csv
import json
import os
import sys
import threading
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Optional, Tuple

# --- –ø–æ–¥–¥–µ—Ä–∂–∫–∞ –∑–∞–ø—É—Å–∫–∞ –∏ –∫–∞–∫ –ø–∞–∫–µ—Ç–∞, –∏ –∫–∞–∫ —Å–∫—Ä–∏–ø—Ç–∞ ---
try:
    from .model import Place
    from .scrape_reviews import (
        scrape_place_reviews,
        MAX_RETRIES_PER_PLACE,
        REVIEW_LANGUAGE,
        DEBUG_SELECTORS,
        CODE_VERSION,
    )
except ImportError:
    from model import Place  # type: ignore
    from scrape_reviews import (  # type: ignore
        scrape_place_reviews,
        MAX_RETRIES_PER_PLACE,
        REVIEW_LANGUAGE,
        DEBUG_SELECTORS,
        CODE_VERSION,
    )

# --- —Ñ–æ—Ä–º–∞—Ç –≤—ã—Ö–æ–¥–Ω–æ–≥–æ CSV ---
OUT_HEADER = [
    "Beach ID","Place","Category","Categories",
    "Place (UI)","Place URL","Input URL","Review ID","Review URL",
    "Rating","Date","Author","Author URL","Author Photo",
    "Is Local Guide","Text","Photo URLs (list)","RawReview"
]



def load_places(csv_path: str) -> List[Place]:
    """
    –ß–∏—Ç–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –º–µ—Å—Ç –∏–∑ CSV.
    –û–∂–∏–¥–∞–µ–º—ã–µ –ø–æ–ª—è: beach_id, place_id, name, category, categories, polygon_name, place_url (–ø–æ—Å–ª–µ–¥–Ω–∏–µ —Ç—Ä–∏ ‚Äî –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ).
    –ü–æ–ª–µ `categories` ‚Äî JSON-–º–∞—Å—Å–∏–≤ —Å—Ç—Ä–æ–∫ (–Ω–∞–ø—Ä. ["Restaurant","Bar"]).
    """
    import json
    places: List[Place] = []

    def _parse_categories(val: Optional[str]):
        if not val:
            return None
        s = (val or "").strip()
        # –ü—ã—Ç–∞–µ–º—Å—è —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –∫–∞–∫ JSON
        try:
            arr = json.loads(s)
            if isinstance(arr, list):
                cleaned = [str(x).strip() for x in arr if str(x).strip()]
                return tuple(cleaned) if cleaned else None
        except Exception:
            pass
        # –§–æ–ª–±—ç–∫: –≤—Ä—É—á–Ω—É—é (–µ—Å–ª–∏ –ø—Ä–∏—à–ª–æ —Ç–∏–ø–∞: ["A","B"] –∏–ª–∏ 'A, B')
        s2 = s.strip().strip("[]")
        if not s2:
            return None
        parts = [p.strip().strip('"').strip("'") for p in s2.split(",")]
        parts = [p for p in parts if p]
        return tuple(parts) if parts else None

    with open(csv_path, encoding="utf-8") as f:
        rdr = csv.DictReader(f)
        for r in rdr:
            beach_id = (r.get("beach_id") or r.get("Beach ID") or "").strip()
            place_id = (r.get("place_id") or r.get("Place ID") or "").strip()
            name = (r.get("name") or r.get("Place") or "").strip()
            polygon_name = (r.get("polygon_name") or r.get("Polygon") or "").strip() or None
            place_url = (r.get("place_url") or r.get("Place URL") or "").strip() or None

            # –ù–æ–≤—ã–µ –ø–æ–ª—è
            category = (r.get("category") or r.get("Category") or "").strip() or None
            categories_raw = (r.get("categories") or r.get("Categories") or "").strip() or None
            categories = _parse_categories(categories_raw)

            if place_id and name:
                places.append(Place(
                    beach_id=beach_id,
                    place_id=place_id,
                    name=name,
                    polygon_name=polygon_name,
                    place_url=place_url,
                    category=category,
                    categories=categories,
                ))
            else:
                # –ø–æ–¥—Å–≤–µ—Ç–∏–º –±–∏—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏, –µ—Å–ª–∏ DEBUG –≤–∫–ª—é—á–µ–Ω
                if DEBUG_SELECTORS:
                    print(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—Å–∫ —Å—Ç—Ä–æ–∫–∏ –±–µ–∑ place_id/name: {r}")

    return places


def load_proxies(file_path: str) -> List[str]:
    """–ß–∏—Ç–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –ø—Ä–æ–∫—Å–∏ –ø–æ –æ–¥–Ω–æ–º—É –Ω–∞ —Å—Ç—Ä–æ–∫—É. –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è http(s)://, socks5://, socks5h://, —Å –ª–æ–≥–∏–Ω–æ–º/–ø–∞—Ä–æ–ª–µ–º."""
    if not file_path:
        return []
    if not os.path.exists(file_path):
        print(f"‚ÑπÔ∏è –§–∞–π–ª –ø—Ä–æ–∫—Å–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path} ‚Äî —Ä–∞–±–æ—Ç–∞–µ–º –±–µ–∑ –ø—Ä–æ–∫—Å–∏.")
        return []
    proxies: List[str] = []
    with open(file_path, encoding="utf-8") as f:
        for line in f:
            s = (line or "").strip()
            if not s or s.startswith("#"):
                continue
            proxies.append(s)
    return proxies


def process_one(idx: int,
                place: Place,
                proxy_url: Optional[str],
                lock: threading.Lock,
                writer: csv.DictWriter,
                out_path: str) -> None:
    """
    –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ –º–µ—Å—Ç–∞: –ø–∞—Ä—Å–∏–Ω–≥ + –∑–∞–ø–∏—Å—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ CSV (–ø–æ–¥ Lock).
    –ü–µ—Ä–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º –≥–æ—Ç–æ–≤—É—é —Ñ—É–Ω–∫—Ü–∏—é scrape_place_reviews().
    """
    acc = []
    last_err: Optional[Exception] = None

    for attempt in range(1, MAX_RETRIES_PER_PLACE + 1):
        try:
            rows = scrape_place_reviews(place, proxy_url=proxy_url, debug=True)
            acc = rows
            break
        except Exception as e:
            last_err = e
            print(f"[{idx}] ERROR {place.name}: {e}")
            time.sleep(0.8 * attempt)

    if not acc:
        print(f"[{idx}] FAIL {place.name}: –ø—É—Å—Ç–æ –ø–æ—Å–ª–µ {MAX_RETRIES_PER_PLACE} –ø–æ–ø—ã—Ç–æ–∫.")
        return

    # –ó–∞–ø–∏—Å—å –≤ —Ñ–∞–π–ª –ø–æ–¥ –±–ª–æ–∫–∏—Ä–æ–≤–∫–æ–π
        # –∑–∞–ø–∏—Å—å –≤ —Ñ–∞–π–ª –ø–æ –º–µ—Ä–µ –ø–æ—Å—Ç—É–ø–ª–µ–Ω–∏—è
    with lock:
        for r in acc:
            r2 = dict(r)

            # –î–æ–±–∞–≤–∏–º category/categories –∏–∑ Place
            r2.setdefault("Category", getattr(place, "category", None))
            cats = getattr(place, "categories", None)
            if isinstance(cats, tuple):
                r2.setdefault("Categories", json.dumps(list(cats), ensure_ascii=False))
            elif isinstance(cats, list):
                r2.setdefault("Categories", json.dumps(cats, ensure_ascii=False))
            else:
                r2.setdefault("Categories", None)

            if isinstance(r2.get("Photo URLs (list)"), list):
                r2["Photo URLs (list)"] = json.dumps(r2["Photo URLs (list)"], ensure_ascii=False)
            if r2.get("RawReview") is not None:
                try:
                    r2["RawReview"] = json.dumps(r2["RawReview"], ensure_ascii=False)
                except Exception:
                    r2["RawReview"] = None

            writer.writerow(r2)

        try:
            sys.stdout.flush()
        except Exception:
            pass

    print(f"[{idx}] {place.name}: +{len(acc)} (–∏—Ç–æ–≥–æ {len(acc)}) –æ—Ç–∑—ã–≤–æ–≤ (UI lang={REVIEW_LANGUAGE})")


def run_worker(worker_id: int,
               jobs: List[Tuple[int, Place]],
               proxy_url: Optional[str],
               lock: threading.Lock,
               writer: csv.DictWriter,
               out_path: str) -> None:
    """
    –í–æ—Ä–∫–µ—Ä –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Å–≤–æ—é –ø–æ—Ä—Ü–∏—é –º–µ—Å—Ç –Ω–∞ –û–î–ù–û–ú –∑–∞–∫—Ä–µ–ø–ª—ë–Ω–Ω–æ–º –ø—Ä–æ–∫—Å–∏.
    jobs: —Å–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π (—Å–∫–≤–æ–∑–Ω–æ–π –∏–Ω–¥–µ–∫—Å, Place).
    """
    if proxy_url:
        print(f"üë§ –í–æ—Ä–∫–µ—Ä #{worker_id}: –ø—Ä–æ–∫—Å–∏ = {proxy_url}")
    else:
        print(f"üë§ –í–æ—Ä–∫–µ—Ä #{worker_id}: –±–µ–∑ –ø—Ä–æ–∫—Å–∏")
    for idx, place in jobs:
        process_one(idx, place, proxy_url, lock, writer, out_path)


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--in", dest="inp", required=True, help="CSV —Å–æ —Å–ø–∏—Å–∫–æ–º –º–µ—Å—Ç (beach_id,place_id,name,...)")
    parser.add_argument("--out", dest="out", required=True, help="CSV –¥–ª—è –∑–∞–ø–∏—Å–∏ –æ—Ç–∑—ã–≤–æ–≤")
    parser.add_argument("--threads", type=int, default=1, help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Ç–æ–∫–æ–≤")
    parser.add_argument("--proxies", default="proxies.txt",
                        help="–§–∞–π–ª —Å –ø—Ä–æ–∫—Å–∏ (–ø–æ –æ–¥–Ω–æ–º—É –Ω–∞ —Å—Ç—Ä–æ–∫—É, http(s)/socks5(h)://user:pass@host:port)")
    args = parser.parse_args()

    places = load_places(args.inp)
    if not places:
        print("–ù–µ—Ç –≤—Ö–æ–¥–Ω—ã—Ö –º–µ—Å—Ç. –ü—Ä–æ–≤–µ—Ä—å CSV.")
        return

    proxies = load_proxies(args.proxies)
    print(f"Scraper version: {CODE_VERSION}")
    print(f"–í—Å–µ–≥–æ –º–µ—Å—Ç: {len(places)}; –ø–æ—Ç–æ–∫–æ–≤ (–∑–∞–ø—Ä–æ—à–µ–Ω–æ): {args.threads}; —Ñ–∞–π–ª –≤—ã–≤–æ–¥–∞: {args.out}")

    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ñ–∞–π–ª–∞ –≤—ã–≤–æ–¥–∞ –∏ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è –∑–∞–ø–∏—Å–∏
    lock = threading.Lock()
    out_dir = os.path.dirname(os.path.abspath(args.out)) or "."
    os.makedirs(out_dir, exist_ok=True)

    need_header = not os.path.exists(args.out) or os.path.getsize(args.out) == 0
    out_f = open(args.out, "a", encoding="utf-8", newline="")
    writer = csv.DictWriter(out_f, fieldnames=OUT_HEADER)
    if need_header:
        writer.writeheader()

    # –ó–∞–¥–∞—á–∏ —Å–æ —Å–∫–≤–æ–∑–Ω–æ–π –Ω—É–º–µ—Ä–∞—Ü–∏–µ–π
    tasks: List[Tuple[int, Place]] = list(enumerate(places, start=1))

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–µ–∞–ª—å–Ω–æ–µ —á–∏—Å–ª–æ –≤–æ—Ä–∫–µ—Ä–æ–≤
    wanted = max(1, int(args.threads))
    if proxies:
        workers = min(wanted, len(proxies))
        if wanted > len(proxies):
            print(f"‚ö†Ô∏è –ó–∞–ø—Ä–æ—à–µ–Ω–æ –ø–æ—Ç–æ–∫–æ–≤: {wanted}, –Ω–æ –ø—Ä–æ–∫—Å–∏: {len(proxies)}. "
                  f"–£—Ä–µ–∂—É –¥–æ {workers}, —á—Ç–æ–±—ã –∫–∞–∂–¥–æ–º—É –ø–æ—Ç–æ–∫—É –¥–æ—Å—Ç–∞–ª—Å—è —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –ø—Ä–æ–∫—Å–∏.")
    else:
        workers = wanted

    # –†–∞–∑–±–∏–≤–∞–µ–º –∑–∞–¥–∞—á–∏ –ø–æ –≤–æ—Ä–∫–µ—Ä–∞–º —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ (—à–∞–≥–æ–≤–∞—è —Ä–∞–∑–±–∏–≤–∫–∞)
    # –ü—Ä–∏ workers > len(tasks) —á–∞—Å—Ç—å —á–∞–Ω–∫–æ–≤ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—É—Å—Ç–æ–π ‚Äî —ç—Ç–æ –æ–∫.
    chunks: List[List[Tuple[int, Place]]] = [tasks[i::workers] for i in range(workers)]

    print(
        f"‚Üí –°—Ç–∞—Ä—Ç—É—é {workers} –ø–æ—Ç–æ–∫(–∞/–æ–≤). –ö–∞–∂–¥–æ–º—É –ø–æ—Ç–æ–∫—É –∑–∞–∫—Ä–µ–ø–ª—è—é —Å–≤–æ–π –ø—Ä–æ–∫—Å–∏."
        if proxies else
        f"‚Üí –°—Ç–∞—Ä—Ç—É—é {workers} –ø–æ—Ç–æ–∫(–∞/–æ–≤) –±–µ–∑ –ø—Ä–æ–∫—Å–∏."
    )

    try:
        with ThreadPoolExecutor(max_workers=workers) as ex:
            futs = []
            for w_id in range(workers):
                proxy_for_worker = proxies[w_id] if proxies else None
                futs.append(ex.submit(run_worker, w_id, chunks[w_id], proxy_for_worker, lock, writer, args.out))
            for _ in as_completed(futs):
                pass
    finally:
        out_f.flush()
        out_f.close()

    print("‚úÖ –ì–æ—Ç–æ–≤–æ.")


if __name__ == "__main__":
    main()
